{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c580b1",
   "metadata": {},
   "source": [
    "## qwen MOE trace and analysis module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ca178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import datasets\n",
    "# from datasets import load_dataset\n",
    "# data_id = \"wikimedia/wikipedia\"\n",
    "# sub_set_id = \"20231101.zh-classical\"\n",
    "# split = \"train\"\n",
    "# dataset = load_dataset(data_id, sub_set_id, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6d8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_v1 import Qwen3MoeDecoderLayerTimed as v1Timed\n",
    "from qwen_v2 import Qwen3MoeDecoderLayerTimed as v2Timed\n",
    "\n",
    "version = 'v2'\n",
    "\n",
    "def modify_qwen3_moe_block(type__: str):\n",
    "    from transformers.models.qwen3_moe import modeling_qwen3_moe as qmoe\n",
    "    if type__ == 'v1':\n",
    "        qmoe.Qwen3MoeDecoderLayer = v1Timed\n",
    "    elif type__ == 'v2':\n",
    "        qmoe.Qwen3MoeDecoderLayer = v2Timed\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "modify_qwen3_moe_block(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02eeb8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dfa9fa09ab46da8406b8c1c1fcbe28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen3MoeForCausalLM(\n",
       "  (model): Qwen3MoeModel(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x Qwen3MoeDecoderLayer(\n",
       "        (self_attn): Qwen3MoeAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MoeSparseMoeBlock(\n",
       "          (gate): Linear(in_features=2048, out_features=128, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-127): 128 x Qwen3MoeMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (down_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3MoeRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3MoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "# model_name = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "cfg = AutoConfig.from_pretrained(model_name)\n",
    "if version == 'v2':\n",
    "    from common import init_timer_registry\n",
    "    init_timer_registry(\n",
    "        num_layers=cfg.num_hidden_layers, keep_history=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d7f34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[94344,   279,  2804, 16948]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [\"explain the qwen\"]\n",
    "tokenizer.padding_side = \"left\"\n",
    "input_001 = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "input_001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a3a03f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PyTorchBenchmark' from 'transformers' (/home/lkx/data/qwen_moe/.venv/lib/python3.13/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyTorchBenchmark, PyTorchBenchmarkArguments\n\u001b[32m      2\u001b[39m args = PyTorchBenchmarkArguments(models=[model_name], batch_sizes=[\u001b[32m8\u001b[39m], sequence_lengths=[\u001b[32m8\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m512\u001b[39m])\n\u001b[32m      3\u001b[39m benchmark = PyTorchBenchmark(args)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PyTorchBenchmark' from 'transformers' (/home/lkx/data/qwen_moe/.venv/lib/python3.13/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments\n",
    "args = PyTorchBenchmarkArguments(models=[model_name], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512])\n",
    "benchmark = PyTorchBenchmark(args)\n",
    "benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21728d4a",
   "metadata": {},
   "source": [
    "## for v1 to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dfb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "import torch\n",
    "import qwen_v1\n",
    "# warm up\n",
    "with torch.no_grad():\n",
    "    model_output = model(**input_001, use_cache=True)\n",
    "qwen_v1.reset_timers()\n",
    "next_token = torch.argmax(model_output.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "past_kv = model_output.past_key_values\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=False, with_stack=True, profile_memory=False\n",
    ") as prof:\n",
    "    with torch.no_grad():\n",
    "        _ = model(next_token, past_key_values=past_kv, use_cache=True)\n",
    "qwen_v1.show_res()\n",
    "prof.export_chrome_trace(\"trace/trace_qwen3_moe_v1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3142a5",
   "metadata": {},
   "source": [
    "## for v2 to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894b867a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerRegistry(num_layers=48, keep_history=True, timers={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import common\n",
    "common._TREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846bbd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Per-layer (ms) ===\n",
      "layer\tattn(PF)\tmlp(PF)\tgating(PF)\tsoftmax(PF)\texpert(PF)\t||\tattn(DEC)\tmlp(DEC)\tgating(DEC)\tsoftmax(DEC)\texpert(DEC)\n",
      "L00\t0.886\t\t8.584\t\t0.087\t\t0.138\t\t8.015\t\t||\t56.022\t\t168.423\t\t5.368\t\t8.552\t\t142.410\n",
      "L01\t0.814\t\t6.696\t\t0.082\t\t0.135\t\t6.293\t\t||\t47.158\t\t163.952\t\t4.880\t\t8.239\t\t139.683\n",
      "L02\t0.786\t\t7.320\t\t0.081\t\t0.134\t\t6.918\t\t||\t46.961\t\t163.946\t\t4.800\t\t8.293\t\t139.797\n",
      "L03\t0.796\t\t7.339\t\t0.081\t\t0.136\t\t6.930\t\t||\t47.408\t\t163.375\t\t4.827\t\t8.249\t\t139.285\n",
      "L04\t0.781\t\t7.328\t\t0.083\t\t0.134\t\t6.928\t\t||\t46.920\t\t163.220\t\t4.754\t\t8.260\t\t139.233\n",
      "L05\t0.776\t\t6.855\t\t0.079\t\t0.134\t\t6.458\t\t||\t46.717\t\t162.910\t\t4.803\t\t8.252\t\t138.886\n",
      "L06\t0.776\t\t7.071\t\t0.081\t\t0.131\t\t6.678\t\t||\t46.775\t\t162.331\t\t4.723\t\t8.241\t\t138.373\n",
      "L07\t0.781\t\t6.807\t\t0.090\t\t0.144\t\t6.391\t\t||\t46.746\t\t162.504\t\t4.777\t\t8.234\t\t138.524\n",
      "L08\t0.781\t\t7.340\t\t0.078\t\t0.141\t\t6.933\t\t||\t46.668\t\t162.604\t\t4.720\t\t8.224\t\t138.677\n",
      "L09\t0.837\t\t6.519\t\t0.083\t\t0.143\t\t6.099\t\t||\t46.669\t\t163.626\t\t4.753\t\t8.201\t\t139.692\n",
      "L10\t0.773\t\t7.579\t\t0.085\t\t0.140\t\t7.168\t\t||\t47.033\t\t162.654\t\t4.760\t\t8.221\t\t138.651\n",
      "L11\t0.810\t\t6.857\t\t0.080\t\t0.134\t\t6.455\t\t||\t49.745\t\t162.829\t\t4.803\t\t8.312\t\t138.662\n",
      "L12\t0.787\t\t6.646\t\t0.079\t\t0.134\t\t6.247\t\t||\t46.658\t\t162.799\t\t4.737\t\t8.260\t\t138.832\n",
      "L13\t0.772\t\t6.416\t\t0.077\t\t0.132\t\t6.023\t\t||\t46.479\t\t162.759\t\t4.767\t\t8.192\t\t138.844\n",
      "L14\t0.778\t\t7.051\t\t0.076\t\t0.130\t\t6.661\t\t||\t46.817\t\t162.820\t\t4.751\t\t8.310\t\t138.867\n",
      "L15\t0.785\t\t6.741\t\t0.079\t\t0.138\t\t6.338\t\t||\t46.564\t\t161.989\t\t4.759\t\t8.228\t\t138.054\n",
      "L16\t0.792\t\t7.306\t\t0.082\t\t0.133\t\t6.903\t\t||\t46.444\t\t164.101\t\t4.759\t\t8.305\t\t140.025\n",
      "L17\t0.778\t\t7.660\t\t0.078\t\t0.144\t\t7.244\t\t||\t46.514\t\t166.059\t\t4.774\t\t8.310\t\t138.914\n",
      "L18\t0.855\t\t6.471\t\t0.084\t\t0.172\t\t6.019\t\t||\t46.458\t\t163.034\t\t4.736\t\t8.271\t\t139.004\n",
      "L19\t0.779\t\t6.192\t\t0.078\t\t0.135\t\t5.796\t\t||\t46.588\t\t162.389\t\t4.775\t\t8.249\t\t138.343\n",
      "L20\t0.774\t\t6.696\t\t0.079\t\t0.139\t\t6.297\t\t||\t46.548\t\t163.174\t\t4.747\t\t8.259\t\t139.206\n",
      "L21\t0.767\t\t6.247\t\t0.084\t\t0.131\t\t5.843\t\t||\t47.050\t\t162.877\t\t4.762\t\t8.271\t\t138.823\n",
      "L22\t0.769\t\t5.967\t\t0.077\t\t0.130\t\t5.573\t\t||\t46.676\t\t162.260\t\t4.692\t\t8.180\t\t138.445\n",
      "L23\t0.795\t\t6.149\t\t0.076\t\t0.134\t\t5.757\t\t||\t46.310\t\t162.994\t\t4.747\t\t8.269\t\t138.973\n",
      "L24\t0.778\t\t5.934\t\t0.082\t\t0.137\t\t5.521\t\t||\t46.508\t\t162.464\t\t4.670\t\t8.167\t\t138.630\n",
      "L25\t0.788\t\t5.910\t\t0.075\t\t0.130\t\t5.521\t\t||\t46.671\t\t163.293\t\t4.737\t\t8.175\t\t139.365\n",
      "L26\t0.787\t\t6.402\t\t0.083\t\t0.146\t\t5.987\t\t||\t47.127\t\t162.516\t\t4.762\t\t8.298\t\t138.459\n",
      "L27\t0.772\t\t6.447\t\t0.077\t\t0.143\t\t6.044\t\t||\t46.606\t\t162.326\t\t4.737\t\t8.193\t\t138.403\n",
      "L28\t0.769\t\t6.864\t\t0.076\t\t0.131\t\t6.471\t\t||\t49.984\t\t163.709\t\t4.754\t\t8.340\t\t139.528\n",
      "L29\t0.769\t\t6.529\t\t0.076\t\t0.130\t\t6.140\t\t||\t46.569\t\t162.556\t\t4.745\t\t8.258\t\t138.560\n",
      "L30\t0.776\t\t7.139\t\t0.076\t\t0.131\t\t6.747\t\t||\t46.456\t\t163.018\t\t4.709\t\t8.221\t\t139.084\n",
      "L31\t0.788\t\t6.378\t\t0.076\t\t0.130\t\t5.993\t\t||\t46.515\t\t162.018\t\t4.759\t\t8.144\t\t138.192\n",
      "L32\t0.773\t\t7.164\t\t0.078\t\t0.135\t\t6.764\t\t||\t46.589\t\t162.934\t\t4.691\t\t8.212\t\t139.062\n",
      "L33\t0.795\t\t6.372\t\t0.080\t\t0.136\t\t5.966\t\t||\t46.822\t\t162.675\t\t4.805\t\t8.234\t\t138.670\n",
      "L34\t0.773\t\t7.094\t\t0.084\t\t0.134\t\t6.684\t\t||\t46.536\t\t162.540\t\t4.741\t\t8.231\t\t138.509\n",
      "L35\t0.778\t\t6.684\t\t0.077\t\t0.134\t\t6.289\t\t||\t46.736\t\t163.381\t\t4.799\t\t8.301\t\t139.292\n",
      "L36\t0.781\t\t6.176\t\t0.088\t\t0.138\t\t5.753\t\t||\t46.441\t\t162.555\t\t4.709\t\t8.208\t\t138.621\n",
      "L37\t0.796\t\t5.813\t\t0.081\t\t0.135\t\t5.403\t\t||\t46.771\t\t163.341\t\t4.803\t\t8.250\t\t139.310\n",
      "L38\t0.771\t\t6.214\t\t0.081\t\t0.135\t\t5.804\t\t||\t46.712\t\t162.220\t\t4.755\t\t8.253\t\t138.262\n",
      "L39\t0.771\t\t7.108\t\t0.077\t\t0.131\t\t6.713\t\t||\t46.447\t\t162.893\t\t4.763\t\t8.222\t\t138.921\n",
      "L40\t0.762\t\t7.099\t\t0.077\t\t0.130\t\t6.706\t\t||\t46.845\t\t163.309\t\t4.825\t\t8.370\t\t139.083\n",
      "L41\t0.767\t\t7.272\t\t0.075\t\t0.130\t\t6.885\t\t||\t46.512\t\t162.908\t\t4.783\t\t8.198\t\t138.919\n",
      "L42\t0.771\t\t7.562\t\t0.079\t\t0.131\t\t7.158\t\t||\t46.810\t\t162.525\t\t4.803\t\t8.284\t\t138.451\n",
      "L43\t0.789\t\t6.859\t\t0.082\t\t0.132\t\t6.449\t\t||\t46.358\t\t164.927\t\t4.796\t\t8.268\t\t138.433\n",
      "L44\t0.772\t\t7.021\t\t0.077\t\t0.131\t\t6.628\t\t||\t46.589\t\t163.325\t\t4.733\t\t8.238\t\t139.356\n",
      "L45\t0.774\t\t7.199\t\t0.077\t\t0.132\t\t6.805\t\t||\t46.670\t\t162.062\t\t4.780\t\t8.255\t\t138.095\n",
      "L46\t0.781\t\t7.503\t\t0.077\t\t0.132\t\t7.111\t\t||\t46.446\t\t162.844\t\t4.678\t\t8.199\t\t138.918\n",
      "L47\t0.762\t\t8.065\t\t0.075\t\t0.130\t\t7.669\t\t||\t46.821\t\t162.512\t\t4.802\t\t8.213\t\t138.451\n",
      "\n",
      "=== Totals (ms) ===\n",
      "attn    prefill=37.671 ms\tdecode=2256.470 ms\tall=2294.142 ms\n",
      "mlp     prefill=328.643 ms\tdecode=7828.480 ms\tall=8157.123 ms\n",
      "gating  prefill=3.822 ms\tdecode=229.112 ms\tall=232.934 ms\n",
      "softmax prefill=6.499 ms\tdecode=396.115 ms\tall=402.614 ms\n",
      "expert  prefill=309.179 ms\tdecode=6668.779 ms\tall=6977.958 ms\n",
      "\n",
      "mlp_ratio_over_(attn+mlp) = 78.05%\n",
      "\n",
      "(Notes) prefill=首段批量计算；decode=逐token阶段（利用 KV cache）。\n",
      "Timing is device-side via CUDA events; each segment synchronized at stop for accuracy.\n"
     ]
    }
   ],
   "source": [
    "import common\n",
    "import torch\n",
    "# warm up\n",
    "\n",
    "common.warmup_model(model, tokenizer, text_list, 10)\n",
    "\n",
    "init_timer_registry(model.config.num_hidden_layers, keep_history=True)\n",
    "with torch.no_grad():\n",
    "    _ = model(**input_001)  # prefill\n",
    "    _ = model.generate(**input_001, max_new_tokens=128)  # decode\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 打印结果\n",
    "common.print_timers_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
