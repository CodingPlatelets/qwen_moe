{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0dd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oss_v1 import GptOssDecoderLayer as v1_timed\n",
    "\n",
    "def modify_oss_decoder_layer(__type: str):\n",
    "    if __type == \"v1\":\n",
    "        from transformers.models.gpt_oss import modeling_gpt_oss as gmoe\n",
    "        gmoe.GptOssDecoderLayer = v1_timed\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown OSS decoder layer type: {__type}\")\n",
    "modify_oss_decoder_layer(\"v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4403f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3b5e592f2147a191d857687cfbb01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GptOssForCausalLM(\n",
       "  (model): GptOssModel(\n",
       "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GptOssDecoderLayer(\n",
       "        (self_attn): GptOssAttention(\n",
       "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
       "        )\n",
       "        (mlp): GptOssMLPV1(\n",
       "          (router): GptOssTopKRouter()\n",
       "          (experts): GptOssExperts()\n",
       "        )\n",
       "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "    (rotary_emb): GptOssRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Mxfp4Config\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "quantization_config = Mxfp4Config(dequantize=False)\n",
    "cfg = AutoConfig.from_pretrained(model_id)\n",
    "from common import init_timer_registry\n",
    "init_timer_registry(cfg.num_hidden_layers, keep_history=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=quantization_config, config=cfg, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc04a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Per-layer (ms) ===\n",
      "layer\tattn(PF)\tmlp(PF)\tgating(PF)\tsoftmax(PF)\texpert(PF)\t||\tattn(DEC)\tmlp(DEC)\tgating(DEC)\tsoftmax(DEC)\texpert(DEC)\n",
      "L00\t1.344\t\t2.459\t\t0.280\t\t0.000\t\t1.921\t\t||\t44.423\t\t73.316\t\t8.839\t\t0.000\t\t58.724\n",
      "L01\t1.093\t\t2.335\t\t0.262\t\t0.000\t\t1.906\t\t||\t32.604\t\t71.425\t\t7.991\t\t0.000\t\t58.354\n",
      "L02\t1.116\t\t2.306\t\t0.249\t\t0.000\t\t1.893\t\t||\t32.788\t\t71.328\t\t7.941\t\t0.000\t\t58.316\n",
      "L03\t1.195\t\t2.312\t\t0.250\t\t0.000\t\t1.899\t\t||\t32.293\t\t71.166\t\t7.957\t\t0.000\t\t58.282\n",
      "L04\t1.086\t\t2.313\t\t0.247\t\t0.000\t\t1.892\t\t||\t32.099\t\t70.990\t\t7.792\t\t0.000\t\t58.286\n",
      "L05\t1.090\t\t2.297\t\t0.247\t\t0.000\t\t1.894\t\t||\t31.462\t\t70.780\t\t7.657\t\t0.000\t\t58.161\n",
      "L06\t1.116\t\t2.301\t\t0.245\t\t0.000\t\t1.896\t\t||\t32.028\t\t71.068\t\t7.764\t\t0.000\t\t58.319\n",
      "L07\t1.191\t\t2.344\t\t0.279\t\t0.000\t\t1.899\t\t||\t32.087\t\t71.615\t\t7.936\t\t0.000\t\t58.485\n",
      "L08\t1.116\t\t2.301\t\t0.246\t\t0.000\t\t1.893\t\t||\t32.352\t\t70.866\t\t7.700\t\t0.000\t\t58.234\n",
      "L09\t1.062\t\t2.338\t\t0.259\t\t0.000\t\t1.895\t\t||\t31.356\t\t71.189\t\t7.896\t\t0.000\t\t58.296\n",
      "L10\t1.096\t\t2.306\t\t0.249\t\t0.000\t\t1.896\t\t||\t32.010\t\t70.685\t\t7.661\t\t0.000\t\t58.115\n",
      "L11\t1.115\t\t2.400\t\t0.266\t\t0.000\t\t1.892\t\t||\t31.724\t\t72.015\t\t7.643\t\t0.000\t\t58.290\n",
      "L12\t1.124\t\t2.315\t\t0.256\t\t0.000\t\t1.896\t\t||\t32.306\t\t70.859\t\t7.728\t\t0.000\t\t58.213\n",
      "L13\t1.048\t\t2.307\t\t0.252\t\t0.000\t\t1.893\t\t||\t31.273\t\t70.731\t\t7.719\t\t0.000\t\t58.182\n",
      "L14\t1.116\t\t2.314\t\t0.246\t\t0.000\t\t1.902\t\t||\t31.727\t\t70.627\t\t7.639\t\t0.000\t\t58.158\n",
      "L15\t1.040\t\t2.310\t\t0.252\t\t0.000\t\t1.897\t\t||\t31.214\t\t71.092\t\t7.612\t\t0.000\t\t58.190\n",
      "L16\t1.096\t\t2.294\t\t0.249\t\t0.000\t\t1.892\t\t||\t31.817\t\t70.642\t\t7.612\t\t0.000\t\t58.140\n",
      "L17\t1.083\t\t2.299\t\t0.243\t\t0.000\t\t1.895\t\t||\t31.171\t\t70.610\t\t7.589\t\t0.000\t\t58.162\n",
      "L18\t1.131\t\t2.320\t\t0.248\t\t0.000\t\t1.895\t\t||\t31.789\t\t70.570\t\t7.625\t\t0.000\t\t58.098\n",
      "L19\t1.081\t\t2.290\t\t0.240\t\t0.000\t\t1.892\t\t||\t31.105\t\t70.617\t\t7.602\t\t0.000\t\t58.196\n",
      "L20\t1.091\t\t2.328\t\t0.275\t\t0.000\t\t1.898\t\t||\t31.877\t\t70.649\t\t7.624\t\t0.000\t\t58.175\n",
      "L21\t1.096\t\t2.283\t\t0.237\t\t0.000\t\t1.890\t\t||\t30.893\t\t70.441\t\t7.490\t\t0.000\t\t58.124\n",
      "L22\t1.098\t\t2.293\t\t0.246\t\t0.000\t\t1.892\t\t||\t31.670\t\t71.437\t\t7.690\t\t0.000\t\t58.850\n",
      "L23\t1.104\t\t2.318\t\t0.249\t\t0.000\t\t1.898\t\t||\t31.024\t\t70.553\t\t7.585\t\t0.000\t\t58.171\n",
      "\n",
      "=== Totals (ms) ===\n",
      "attn    prefill=26.729 ms\tdecode=775.090 ms\tall=801.820 ms\n",
      "mlp     prefill=55.683 ms\tdecode=1705.273 ms\tall=1760.956 ms\n",
      "gating  prefill=6.072 ms\tdecode=186.294 ms\tall=192.365 ms\n",
      "softmax prefill=0.000 ms\tdecode=0.000 ms\tall=0.000 ms\n",
      "expert  prefill=45.516 ms\tdecode=1398.519 ms\tall=1444.035 ms\n",
      "\n",
      "mlp_ratio_over_(attn+mlp) = 68.71%\n",
      "\n",
      "(Notes) prefill=首段批量计算；decode=逐token阶段（利用 KV cache）。\n",
      "Timing is device-side via CUDA events; each segment synchronized at stop for accuracy.\n"
     ]
    }
   ],
   "source": [
    "import common\n",
    "common._TREG\n",
    "import torch\n",
    "# warm up\n",
    "text_list = [\"explain the qwen\"]\n",
    "tokenizer.padding_side = \"left\"\n",
    "input_001 = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "common.warmup_model(model, tokenizer, text_list, 10)\n",
    "\n",
    "init_timer_registry(model.config.num_hidden_layers, keep_history=True)\n",
    "with torch.no_grad():\n",
    "    _ = model(**input_001)  # prefill\n",
    "    _ = model.generate(**input_001, max_new_tokens=64)  # decode\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 打印结果\n",
    "common.print_timers_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is the difference between mixtral and qwen moe models?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=16384,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
