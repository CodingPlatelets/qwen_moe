{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a782956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkx/data/qwen_moe/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mixtral_v1 import MixtralDecoderLayerTimed\n",
    "\n",
    "\n",
    "def modify_mixtral_moe_block(type__: str):\n",
    "    if type__ == 'decoder':\n",
    "        import transformers.models.mixtral.modeling_mixtral as mix_models\n",
    "        mix_models.MixtralDecoderLayer = MixtralDecoderLayerTimed\n",
    "    else:\n",
    "        pass\n",
    "modify_mixtral_moe_block('decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a01703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:09<00:00,  1.94it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_id = 'mistralai/Mixtral-8x7B-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, dtype=torch.bfloat16, device_map='auto', max_memory={\n",
    "        0: '64GB'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d914bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21.self_attn': 0, 'model.layers.21.block_sparse_moe.gate': 0, 'model.layers.21.block_sparse_moe.experts.0': 0, 'model.layers.21.block_sparse_moe.experts.1': 0, 'model.layers.21.block_sparse_moe.experts.2': 0, 'model.layers.21.block_sparse_moe.experts.3': 0, 'model.layers.21.block_sparse_moe.experts.4': 0, 'model.layers.21.block_sparse_moe.experts.5': 0, 'model.layers.21.block_sparse_moe.experts.6.w1': 0, 'model.layers.21.block_sparse_moe.experts.6.w2': 0, 'model.layers.21.block_sparse_moe.experts.6.w3': 'disk', 'model.layers.21.block_sparse_moe.experts.6.act_fn': 'disk', 'model.layers.21.block_sparse_moe.experts.7': 'disk', 'model.layers.21.input_layernorm': 'disk', 'model.layers.21.post_attention_layernorm': 'disk', 'model.layers.22': 'disk', 'model.layers.23': 'disk', 'model.layers.24': 'disk', 'model.layers.25': 'disk', 'model.layers.26': 'disk', 'model.layers.27': 'disk', 'model.layers.28': 'disk', 'model.layers.29': 'disk', 'model.layers.30': 'disk', 'model.layers.31': 'disk', 'model.norm': 'disk', 'model.rotary_emb': 'disk', 'lm_head': 'disk'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayerTimed(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MixtralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.hf_device_map)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fa3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is the difference between mixtral and mistral?\"\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e06c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=False, with_stack=True, profile_memory=False\n",
    ") as prof:\n",
    "    with torch.no_grad():\n",
    "        # _ = model(next_token, past_key_values=past_kv, use_cache=True)\n",
    "        _ = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=True,\n",
    "        )\n",
    "prof.export_chrome_trace(\"trace/trace_mixtral_64GB.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f328670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   767,   349,   272,  5133,  1444,  6750,   434,   282,   304,\n",
       "          5710,  1650, 28804,    13,    13, 23809, 28725,    13,    13,  1014,\n",
       "         11398,   464, 23350, 28733, 28742,  2825,  1545,   843,  2508,   304,\n",
       "           791, 28719,  3162, 28723,  1263,  2757,  3475, 28719,  3162, 28733,\n",
       "           715, 28809,   349,   264, 20521,   302,   544,   272,  1722, 28725,\n",
       "           304,   272, 11398,  3475, 28719,   392, 28733, 28809,   349,  1307,\n",
       "           354,   521,  6206,   442, 16502,  2330,  1722, 28723,    13,    13,\n",
       "          2324,   541,   938,  1167,   989, 11398,   274,   297,  1581, 11846,\n",
       "         28723,    13, 26388,  2825,   464,   532,  6750,   647,  1312,  5710,\n",
       "          2825,   464, 28728, 20346,  4135,    13, 26388, 28733,   434,   282,\n",
       "         28746,  1791,   347,  9430, 28723,    13, 28755,   392, 28733,  1650,\n",
       "           327,   330,   363, 20346,  5535, 28723,    13,    13, 28737,  3317,\n",
       "           378,  2870,   378,  3081, 28723,    13,    13, 15896,   368,   354,\n",
       "           272,  2747,  3081, 13268,   304,  4870,  4931, 28723,    13,    13,\n",
       "         12069,  1912,   528,   272,  5133,  1444,  6750,   434,   282,   304,\n",
       "           290,  2174,   309,    13,    13, 23809,    13, 26388, 28806, 28707,\n",
       "          6144,  2825,   298,   347,  9430, 28723,    13, 28755,   392,  6144,\n",
       "         28746,  1791,  5453,   264,  9070,   302,  1545, 28723,    13, 28737,\n",
       "          3317,   378,  7263, 28723,    13,    13, 28719,   392,  6144,   327,\n",
       "         11990,    13,    13, 28719,   392,  1650,   327,  5535, 28748,   992,\n",
       "             2]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=10,\n",
    "    do_sample=True,\n",
    ")\n",
    "generate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02ff943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 0,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 0,\n",
       " 'model.layers.2': 0,\n",
       " 'model.layers.3': 0,\n",
       " 'model.layers.4': 0,\n",
       " 'model.layers.5': 0,\n",
       " 'model.layers.6': 0,\n",
       " 'model.layers.7': 0,\n",
       " 'model.layers.8': 0,\n",
       " 'model.layers.9': 0,\n",
       " 'model.layers.10': 0,\n",
       " 'model.layers.11': 0,\n",
       " 'model.layers.12': 0,\n",
       " 'model.layers.13': 0,\n",
       " 'model.layers.14': 0,\n",
       " 'model.layers.15': 0,\n",
       " 'model.layers.16': 0,\n",
       " 'model.layers.17': 0,\n",
       " 'model.layers.18': 0,\n",
       " 'model.layers.19': 0,\n",
       " 'model.layers.20': 'disk',\n",
       " 'model.layers.21': 'disk',\n",
       " 'model.layers.22': 'disk',\n",
       " 'model.layers.23': 'disk',\n",
       " 'model.layers.24': 'disk',\n",
       " 'model.layers.25': 'disk',\n",
       " 'model.layers.26': 'disk',\n",
       " 'model.layers.27': 'disk',\n",
       " 'model.layers.28': 'disk',\n",
       " 'model.layers.29': 'disk',\n",
       " 'model.layers.30': 'disk',\n",
       " 'model.layers.31': 'disk',\n",
       " 'model.norm': 'disk',\n",
       " 'model.rotary_emb': 'disk',\n",
       " 'lm_head': 'disk'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007015ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- They both mean west wind. Mixtral is a mix of the directions mistral and tramontane, the former coming from the north (northwesterly) and the latter from the north east (northeasterly). Neither of the two winds mentioned are local winds (and thus not included in a glossary of local winds); the mistake comes from a website which misquotes Wikipedia, where it says the mixtral is the direction northwesterly and southeasterly (mistral + tramontane).\\n\\nAs mistral is often considered to be west to northwest, the same word often means westwardly.\\n\\nSource: Wikipedia.\\n\\nThis answer is:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = generate_ids[0][model_inputs['input_ids'].shape[1]:]\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "output_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
